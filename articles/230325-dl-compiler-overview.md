---
title: 深層学習コンパイラの設計概観
emoji: 🚌
type: tech
topics: [コンパイラ]
published: false
---

- **概要**:

## はじめに

近年深層学習の目覚ましい進歩が注目を集めています。例えば ChatGPT をはじめとする大規模言語モデルの登場は、社会面でも学術面でも大きな影響を及ぼしました。

その背後で技術の進展を支えているのが、計算機における計算速度の向上です。深層学習ワークロードは、膨大な計算を必要とします。半導体の微細化がほぼ限界に達し、ムーアの法則の終焉が囁かれる今、ソフトウェアによる計算の高速化が喫緊の課題となっています。

本記事では、このような背景のもと発展したドメイン固有コンパイラ技術、とりわけ深層学習コンパイラの設計について言語化することを試みます。本記事は主に [The Deep Learning Compiler: A Comprehensive Survey](https://ieeexplore.ieee.org/document/9222299)[^survey] に基づいています。

:::message
本記事は正確を期していますが、筆者の不勉強のために誤りが含まれる可能性があります。お気づきの点がございましたらコメント頂けますと幸いです。
:::

## 前提: なぜ高速化が必要か?

<!-- TODO: ワークロードという言葉の使用は適切か? -->

高速化が求められる理由として、深層学習ワークロードにおける計算量の多さが挙げられます。例えば、サイズ $224\ \text{px} × 224\ \text{px}$ の画像を 1000 項目に分類する深さ 50 層の畳み込みニューラルネットワークモデルである ResNet-50 の計算量は、$3.8\times 10^9$ 回$/\text{iter}$ にもなります[^resnet]。これは一般的な CPU で計算させた場合、待ち時間を有意に感じられる程度の時間を要します。

タスクが複雑になり層が深くなるほど計算量は増大する一方、ワークロードにはある程度の速度が求められます。そのことは、学習には $10^2\sim 10^3$ 回程の反復を要し、計算時間はモデル開発速度 (ひいてはモデル性能) に大きく影響することから示唆されます。他にも、例えば Web サービスとして成立させるには、ある程度の応答速度が求められること、自動運転等のリアルタイムで推論結果が必要となるワークロードが存在することを考慮すると、実用化には高速化が欠かせません。

## 深層学習ワークロードの特徴

上述の背景の結果、深層学習コンパイラではワークロードの特徴を利用し、従来の汎用コンパイラより踏み込んだ最適化を実行することが重要です。深層学習ワークロードの特徴として、巨視的な構造と微視的な構造の 2 つの階層構造をもつことが挙げられます。

**巨視的な構造**とは、高水準なベクトル・テンソル演算によって構成される構造のことです。この階層において深層学習モデルは、高水準な演算をノードとしデータ依存関係をエッジとする計算グラフによって表されます。

また**微視的な構造**とは、低レベルなスカラ・ベクトル命令を用いて表される構造のことです。この階層は高水準な演算ノード 1 つ 1 つの構造を低レベル命令を用いて表現しており、演算ノード自体ある程度の粒度を有し最適化の対象となります。

このように深層学習ワークロードは 2 層の階層構造をもち、それぞれの構造が以下の特徴をもちます:

### 巨視的な特徴

- **高水準なベクトル・テンソル演算を単位とした粗視化された計算グラフが与えられる**
  → 高水準な演算レベルでの「大域的な」最適化が実行できる
  例: 「大域的な」共通部分式削除、不要式削除、代数的簡約化
- **計算のデータ依存関係が明確である。特に、ポインタやエイリアスが存在しない**
  → 積極的にレジスタを再利用できる
  例: 演算子融合

### 微視的な特徴

- **単一の命令を複数のデータに対し適用する演算が多い**
  → ハードウェア固有のベクトル命令・テンソル命令を活用できる
  例: ハードウェア固有のマッピング
- **分岐がほとんどない**
  → 分岐予測・投機的実行が重要でない
- **データ依存関係が低レベル命令レベルでも明確である**
  → パイプラインをソフトウェアが決定できる・並列性が比較的容易に抽出できる
  例: パイプラインの静的決定によるメモリレイテンシの隠蔽・並列化
- **メモリアクセスパターンが明確である**
  → メモリキャッシュをソフトウェアが決定できる
  例: メモリ割り当て・データのメモリ階層へのマッピング

特筆すべき点として、上述の最適化にはハードウェアとソフトウェアが協調して初めて可能となる最適化が含まれることが挙げられます。

- #### ベクトル命令・テンソル命令をもつハードウェアの活用

  深層学習ワークロードの特徴に合わせ、GPU や TPU といったベクトル演算・行列演算に特化したハードウェアが台頭しています。これらのハードウェアを活かすには、ソフトウェアがハードウェア固有命令を使用できる場所を検出し、スカラ命令をベクトル命令・テンソル命令に置換することが必要です。

  また、これらのハードウェアは一度に多数のデータを処理できる分、計算速度に比べメモリアクセス速度が著しく遅いという短所をもちます。そのため、計算の特徴を利用してメモリアクセスを極力減らすことも求められます。

  **行列行列積**・**畳み込み**といった行列演算は、元々データ再利用性が高いという特徴を持ちます。例えば$n$次正方行列の行列積は、データ数$n^2$に対し計算量は$O(n^3)$であり、1 つのデータを最大$n$回再利用することができます。その結果、メモリキャッシュ計画を最適化することで、高速なデータ供給が可能となります。

  一方**ベクトルの加算**・**ベクトルのスカラー倍**といった要素ごとの演算は、データの再利用を行うことはできません。しかし、要素間のデータ依存性ないことから、他の演算と融合して連続して実行することで、メモリアクセスを最初の 1 回のみに削減することができます。

- #### 高度なキャッシュ・パイプラインをもたないハードウェアの活用

  踏み込んだ最適化として、汎用的なプロセッサの進歩とは対照的に、簡素化したハードウェアを使用することが挙げられます。CPU 等の従来のプロセッサには、汎用的な計算を高速に実行するために、ハードウェアに高度な機能が備わっています。例として、

  - **自動パイプライン**: データ依存関係によってパイプラインが阻害されることを防ぐために、計算結果の変わらない範囲で命令の順序変更を行う仕組み
  - **自動メモリキャッシュ**: メモリアクセスの時間的・空間的近接性を利用して、データ使用範囲を予測し事前にキャッシュを行う仕組み

  が挙げられます。

  しかし深層学習ワークロードは、データ依存関係やメモリアクセスパターンが明確であるという特徴があります。そのため汎用的な高速化の仕組みに頼らず、ソフトウェアによって静的にパイプライン・メモリキャッシュを制御した方が、効率的な計算を実現できます。

  その結果、高度なパイプライン・メモリキャッシュをもたないプロセッサ(アクセラレータ等)を有効活用できることが明らかとなりました。これらのハードウェアを活かすために、深層学習コンパイラにはパイプライン・メモリキャッシュの制御といったこれまでハードウェアが担っていた役割が求められます。

このように、ソフトウェアだけでなくハードウェアも深層学習ワークロードの特徴に合わせて進化しており、深層学習コンパイラのターゲットは CPU・GPU・TPU・アクセラレータと多岐に渡ります。多様化したターゲットに対応するために、深層学習コンパイラには従来の汎用コンパイラにはないタスクの遂行が求められています。

## 深層学習コンパイラの設計

最適化が可能な抽象レベルごとに中間表現を導入
高レベル中間表現・低レベル中間表現の 2 種類の中間表現を使用

### 高レベル中間表現の特徴

高レベル中間表現は高水準なベクトル・テンソル演算とデータ依存関係を表現します。このレベルで計算グラフはハードウェアに依存しない形式で表されます。

#### 実行される最適化

- 「大域的な」コンパイラ最適化: 例えば以下の最適化が含まれます
  - 共通部分式削除: 計算グラフ内で同じ式の計算が複数回現れる場合に、以前の計算結果を再利用することで、同じ式の再計算を回避します
  - 不要式削除: 計算グラフ内で計算結果や副作用が使用されていない演算を削除します
  - 代数的簡約化: 代数的性質 (演算子の結合法則、分配法則、交換法則等) を利用して計算コストの削減を行います。演算強度低減、定数畳み込みをはじめとする従来の簡約化に加え、深層学習ワークロード特有のテンソルに関する簡約化が行われます。テンソルに関する簡約化として以下の例が挙げられます
    - テンソル演算の性質を利用した演算回数削減: 例えば転置した行列の積を行列積の転置に置き換えます:
      $$A^\top B^\top=(BA)^\top$$
    - テンソルの形状変換の最適化: 例えば連続する転置を削除したり、連続する Reshape を 1 つの Reshape にまとめます:
      $$(A^\top)^\top=A,\quad\mathrm{Reshape}(\mathrm{Reshape}(A))=\mathrm{Reshape}(A)$$
    - テンソルの形状変換の順序変更: 例えば Pooling によってテンソルの要素数が減少する場合に、他の演算の前に Pooling を先に実行します:
      $$\mathrm{MaxPool}(A^\top)=(\mathrm{MaxPool}(A))^\top$$
- 演算子融合: 中間結果のメモリ保存を省略し、複数の演算子を単一のカーネルに統合することで実行時間を短縮します。データ共有の改善、中間割り当ての削除、ループネストの統合によってさらなる最適化を促進し、カーネル起動と同期のオーバーヘッドを削減することができます[^tvm]
- データレイアウト変換: 計算グラフにおいてテンソルを保持する最適なデータレイアウトを探索し、レイアウト変換ノードをグラフに挿入します。ここで最適なレイアウトはハードウェア依存しています。例えば GPU 上では NCHW 形式のデータレイアウトにおいて高速に動作します (ここで NCHW とはバッチ内の画像数$N$、高さ方向の画素数$H$、幅方向の画素数$W$、チャンネル数$C$(グレースケールの場合$C=1$、RGB の場合$C=3$)を格納する順番を指します)。他にもアクセラレータの中には、ハードウェア固有命令やキャッシュの都合上より複雑なデータレイアウトを好むものも存在します。

#### その他の役割: 意味解析

- 形状推論
- 暗黙のブロードキャストの明示化

### 低レベル中間表現の特徴

低レベル中間表現はワークロードを低レベル命令を用いて表現します。このレベルではハードウェア依存の最適化も実行されるため、ハードウェア固有の操作を表現できるほど詳細な命令を表現できる必要があります。

#### 実行される最適化

- ハードウェア固有のマッピング: 低レベル中間表現で記述された特定の命令パターンを、ハードウェア上で高度に最適化されたカーネルに置き換えます
- メモリ階層へのアクセスの調整: 計算機のメモリは、容量は大きいが遅延が大きいメモリと、容量は小さいが遅延が小さいキャッシュを用いた階層構造を持ちます。このメモリ階層はプロセッサごとに異なるため、ハードウェア専用の最適化が必要です。CPU ではプリフェッチや効率的にデータを追い出すハードウェア機能によって効率的なメモリアクセスが暗黙的に実現されます。一方 GPU は共有メモリとローカルメモリを有し、メモリ割り当てをコンパイラとハードウェアが担う箇所が混在しています。また高度なキャッシュを持たないアクセラレータでは、コンパイラがデータ再利用性の高いメモリキャッシュを決定します。
- メモリレイテンシの隠蔽: パイプラインを再構成し、メモリアクセスと計算の実行を可能な限り並列化することでメモリアクセスに由来する遅延を隠蔽します。CPU ではハードウェアプリフェッチ、GPU ではワープコンテキストスイッチといったハードウェアの機能によってメモリレイテンシの隠蔽は暗黙的に実現されます。一方高度なパイプラインを持たないアクセラレータの場合は、コンパイラがデータ依存関係を解析し、命令の同期やスケジューリングを実行する必要があります。
- 並列化
  - 2 種類の中間表現
    - Halide ベースの中間表現 (TVM など)
      - 手動スケジューリングと自動パラメタチューニング
      - スケジューリングの指定法として計算とスケジュールの分離を採用 (これは Halide の設計を借りた)
    - Polyhedral IR (TC・MLIR における affine 方言)
      - 多面体コンパイルを利用した自動スケジューリング
- ループ指向の最適化: ループ融合、sliding window、タイル化、ループの並び替え、およびループ展開などがあります
  - ループ融合: 同じ境界を持つループを融合することで、データ再利用性を向上させる
  - Sliding window: Halide で考案された最適化。中心的なコンセプトは、必要なときに値を計算し、必要なくなるまでデータを再利用するというもの
  - タイル化: ループをいくつかのタイルに分割することで、タイルをハードウェアキャッシュに適合させ、タイル内のデータ局所性を向上させる
  - ループの並べ替え: ネストされたループの反復順序を変更することで、メモリアクセスを最適化し、空間局所性を向上させる
  - ループ展開: タイル化されたループの内側のループを展開することで、コンパイラがアグレッシブに命令レベル並列化を適用できるようにする

[^survey]: [The Deep Learning Compiler: A Comprehensive Survey](https://ieeexplore.ieee.org/document/9222299)
[^resnet]: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
[^tvm]: [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)

<!-- 脚注の書き方^[[How to write footnotes](https://zenn.dev/zenn/articles/markdown-guide#%E6%B3%A8%E9%87%88)]。 -->
